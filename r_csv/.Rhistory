return(R_hat)
}
J <- 4
N <- 2000
s_values <- seq(0.001, 1, length.out = 1000)
R_hat_values <- numeric(length(s_values))
for (i in 1:length(s_values)) {
chains <- sequence(J, N, s_values[i])
R_hat_values[i] <- R_hat_value(chains)
}
plot(s_values, R_hat_values, type = 'l', col = 'blue', xlab = 'Standard Deviation (s)', ylab = 'Gelman-Rubin Diagnostic (R-hat)',
main = 'Convergence Assessment over Different Proposal Standard Deviations')
grid()
knitr::opts_chunk$set(echo = TRUE)
yr2006_df <- read.csv("2006.csv")
pdf<-function(x){
return(exp(-abs(x))*0.5)
}
x0 <- as.numeric(readline( prompt="Enter the x0: " ))
N <- as.integer(readline( prompt="Enter the N: " ))
s <- as.numeric(readline( prompt="Enter the s: " ))
algo <- function(x0,N,s){
ch <-c(x0)
for(i in 2:(N+1)){
xstar<- rnorm(1, mean=ch[i-1],sd=s)
r<-pdf(xstar)/pdf(ch[i-1])
u<- runif(1,0,1)
if(log(u)<log(r)){
ch<-c(ch,xstar)
}else{
ch<-c(ch,ch[i-1])}
}
return(ch[-1])
}
d1<-algo(x0,N,s)
d1
mean(d1)
sd(d1)
hist(d1, probability = TRUE, col = "lightblue", breaks=50, xlab = "x")
lines(density(d1), col = "red", lwd = 4)
curve(pdf(x), col = "blue", add = TRUE, lwd = 2)
J<-as.integer(readline(prompt = "Enter J:"))
intial_values <- numeric(0)
for (i in 1:J) {
val<- as.numeric(readline(prompt = paste("Enter intial value for J",i, ": ")))
intial_values <- c(intial_values, val)
}
print(intial_values)
N <- as.integer(readline( prompt="Enter the N: " ))
s <- as.numeric(readline( prompt="Enter the s: " ))
seq_ch<-list( )
for(j in 1:J){
w=algo(intial_values[j],N,s)
seq_ch<-list(seq_ch,w)
}
chains<-function(J,s){
seqs=list()
for(i in 1:J){
w=algo(intial_values[i],N,s)
seqs[[i]]<-w
}
return(seqs)
}
x<-chains(J,s)
#x
M<-c()
for(j in 1:J){
M<-c(M,mean(x[[j]]))
}
M
V<-c()
for(j in 1:J){
vv=var(x[[j]])
V<-c(V,vv)
}
V
W=mean(V)
W
MM=mean(M)
MM
B=var(M)
B
R=(((B+W)/W)**0.5)
R
y<- seq(0.001, 1, by = 0.001)
R_values<-function(){
RR<-c()
for( s in y){
x<-chains(J,s)
M<-c()
for(j in 1:J){
M<-c(M,mean(x[[j]]))
}
V<-c()
for(j in 1:J){
vv=var(x[[j]])
V<-c(V,vv)
}
W=mean(V)
MM=mean(M)
B=var(M)
R=(((B+W)/W)**0.5)
RR<-c(RR,R)
}
return(RR)
}
pp=R_values()
pp
plot(y, pp, type = "l", col = "blue", ylab = "R values", xlab = "s values")
install.packages("dplyr")
install.package("readr")
install.packages("dplyr")
ontime_df <- bind_rows(
read_csv("1994.csv"),
read_csv("1995.csv")
)
knitr::opts_chunk$set(echo = TRUE)
install.packages("mlr3")
install.packages("mlr3learners")
install.packages("mlr3pipelines")
install.packages("mlr3tuning")
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(mlr3tuning)
install.packages("dplyr") #Install a package called 'dplyr' package
library(dplyr) #Load and use 'dplyr' package to handle the Data Frames
Year1998_df <-read.csv("1998.csv") #Import into Data Frames by using read.csv() function
install.packages("dplyr")
Year1998_df <- Year1998_df [c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance', 'Diverted')]
#Filter out unwanted columns
Year1998_df$CRSDepTime <- (Year1998_df$CRSDepTime / 100)
#Divide 'CRSDepTime' values by 100 to leave them in 2 decimal point
Year1998_df$CRSArrTime <- (Year1998_df$CRSArrTime / 100)
#Divide 'CRSArrTime' values by 100 to leave them in 2 decimal point
Year1998_df$Diverted <- factor(Year1998_df$Diverted)
Year1998_df$DayOfWeek <- factor(Year1998_df$DayOfWeek, order = TRUE,
levels = c("1", "2", "3", "4", "5", "6", "7"))
n <- nrow(Year1998_df) #n is the number of rows in 'Year 1998' data frame
train_set1998 <- sample(n, round(0.55*n)) #Training set extracted 55 percent of the number of rows
test_set1998 <- setdiff(1:n, train_set1998) #Test set will have 45 percent of the remaining rows
task1998 <- TaskClassif$new('Year1998_df',
backend = Year1998_df,
target = 'Diverted')
task1998$select(c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance'))
measure1998 <- msr('classif.ce')
task1998
ord_to_int <- po("colapply", applicator = as.integer,
affect_columns = selector_type("ordered"))
learner_lr <- lrn("classif.log_reg")
gc_lr <- po('imputemean') %>>%
po(learner_lr)
glrn_lr <- GraphLearner$new(gc_lr)
glrn_lr$train(task1998, row_ids = train_set1998)
glrn_lr$predict(task1998, row_ids = test_set1998)$score()
Year1999_df <-read.csv("1999.csv") #Import into Data Frames by using read.csv() function
Year1999_df <- Year1999_df [c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance', 'Diverted')]
#Filter out unwanted columns
Year1999_df$CRSDepTime <- (Year1999_df$CRSDepTime / 100)
#Divide 'CRSDepTime' values by 100 to leave them in 2 decimal point
Year1999_df$CRSArrTime <- (Year1999_df$CRSArrTime / 100)
#Divide 'CRSArrTime' values by 100 to leave them in 2 decimal point
Year1999_df$Diverted <- factor(Year1999_df$Diverted)
Year1999_df$DayOfWeek <- factor(Year1999_df$DayOfWeek, order = TRUE,
levels = c("1", "2", "3", "4", "5", "6", "7"))
n <- nrow(Year1999_df) #n is the number of rows in 'Year 1999' data frame
train_set1999 <- sample(n, round(0.55*n)) #Training set extracted 55 percent of the number of rows
test_set1999 <- setdiff(1:n, train_set1999) #Test set will have 45 percent of the remaining rows
task1999 <- TaskClassif$new('Year1999_df',
backend = Year1999_df,
target = 'Diverted')
task1999$select(c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance'))
measure1999 <- msr('classif.ce')
task1999
ord_to_int <- po("colapply", applicator = as.integer,
affect_columns = selector_type("ordered"))
glrn_lr$train(task1999, row_ids = train_set1999)
glrn_lr$predict(task1999, row_ids = test_set1999)$score()
Year2000_df <-read.csv("2000.csv") #Import into Data Frames by using read.csv() function
Year2000_df <- Year2000_df [c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance', 'Diverted')]
#Filter out unwanted columns
Year2000_df$CRSDepTime <- (Year2000_df$CRSDepTime / 100)
#Divide 'CRSDepTime' values by 100 to leave them in 2 decimal point
Year2000_df$CRSArrTime <- (Year2000_df$CRSArrTime / 100)
#Divide 'CRSArrTime' values by 100 to leave them in 2 decimal point
Year2000_df$Diverted <- factor(Year2000_df$Diverted)
Year2000_df$DayOfWeek <- factor(Year2000_df$DayOfWeek, order = TRUE,
levels = c("1", "2", "3", "4", "5", "6", "7"))
n <- nrow(Year2000_df) #n is the number of rows in 'Year 2000' data frame
train_set2000 <- sample(n, round(0.55*n)) #Training set extracted 55 percent of the number of rows
test_set2000 <- setdiff(1:n, train_set2000) #Test set will have 45 percent of the remaining rows
task2000 <- TaskClassif$new('Year2000_df',
backend = Year2000_df,
target = 'Diverted')
task2000$select(c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance'))
measure2000 <- msr('classif.ce')
task2000
ord_to_int <- po("colapply", applicator = as.integer,
affect_columns = selector_type("ordered"))
glrn_lr$train(task2000, row_ids = train_set2000)
glrn_lr$predict(task2000, row_ids = test_set2000)$score()
Year2001_df <-read.csv("2001.csv") #Import into Data Frames by using read.csv() function
Year2001_df <- Year2001_df [c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance', 'Diverted')]
#Filter out unwanted columns
Year2001_df$CRSDepTime <- (Year2001_df$CRSDepTime / 100)
#Divide 'CRSDepTime' values by 100 to leave them in 2 decimal point
Year2001_df$CRSArrTime <- (Year2001_df$CRSArrTime / 100)
#Divide 'CRSArrTime' values by 100 to leave them in 2 decimal point
Year2001_df$Diverted <- factor(Year2001_df$Diverted)
Year2001_df$DayOfWeek <- factor(Year2001_df$DayOfWeek, order = TRUE,
levels = c("1", "2", "3", "4", "5", "6", "7"))
n <- nrow(Year2001_df) #n is the number of rows in 'Year 2001' data frame
train_set2001 <- sample(n, round(0.55*n)) #Training set extracted 55 percent of the number of rows
test_set2001 <- setdiff(1:n, train_set2001) #Test set will have 45 percent of the remaining rows
task2001 <- TaskClassif$new('Year2001_df',
backend = Year2001_df,
target = 'Diverted')
task2001$select(c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance'))
measure2001 <- msr('classif.ce')
task2001
ord_to_int <- po("colapply", applicator = as.integer,
affect_columns = selector_type("ordered"))
glrn_lr$train(task2001, row_ids = train_set2001)
glrn_lr$predict(task2001, row_ids = test_set2001)$score()
Year2002_df <-read.csv("2002.csv") #Import into Data Frames by using read.csv() function
Year2002_df <- Year2002_df [c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance', 'Diverted')]
#Filter out unwanted columns
Year2002_df$CRSDepTime <- (Year2002_df$CRSDepTime / 100)
#Divide 'CRSDepTime' values by 100 to leave them in 2 decimal point
Year2002_df$CRSArrTime <- (Year2002_df$CRSArrTime / 100)
#Divide 'CRSArrTime' values by 100 to leave them in 2 decimal point
Year2002_df$Diverted <- factor(Year2002_df$Diverted)
Year2002_df$DayOfWeek <- factor(Year2002_df$DayOfWeek, order = TRUE,
levels = c("1", "2", "3", "4", "5", "6", "7"))
n <- nrow(Year2002_df) #n is the number of rows in 'Year 2002' data frame
train_set2002 <- sample(n, round(0.55*n)) #Training set extracted 55 percent of the number of rows
test_set2002 <- setdiff(1:n, train_set2002) #Test set will have 45 percent of the remaining rows
task2002 <- TaskClassif$new('Year2002_df',
backend = Year2002_df,
target = 'Diverted')
task2002$select(c('DayOfWeek', 'CRSDepTime', 'CRSArrTime', 'Distance'))
measure2002 <- msr('classif.ce')
task2002
ord_to_int <- po("colapply", applicator = as.integer,
affect_columns = selector_type("ordered"))
glrn_lr$train(task2002, row_ids = train_set2002)
glrn_lr$predict(task2002, row_ids = test_set2002)$score()
Year2003_df <-read.csv("2003.csv") #Import into Data Frames by using read.csv() function
knitr::opts_chunk$set(echo = TRUE)
initial_x <- c(1) #Give x a value of 1
assign('N', 10000) #Set value of N as 10000
assign('s', 1) #Set initial value of s as 1
knitr::opts_chunk$set(echo = TRUE)
initial_x <- c(1) #Give x a value of 1
assign('N', 10000) #Set value of N as 10000
assign('s', 1) #Set initial value of s as 1
x_star <- rnorm(n = 1, mean = initial_x, sd = s)
r = ((1/2*exp(-abs(x_star)))/(1/2*exp(-abs(initial_x))))
u = runif(1, min = 0, max = 1)
if (u < r) x = x_star else x = initial_x
x_df <- data.frame(x)
for (i in 2:N) {
x_star <- rnorm(n = 1, mean = x[i-1], sd = s)
r = (1/2*exp(-abs(x_star)))/(1/2*exp(-abs(x[i-1])))
u = runif(1, min = 0, max = 1)
if (u < r) x[i] = x_star else x[i] = x[i-1]
}
x_df <- data.frame(x)
fx <- (1/2*exp(-abs(x[1])))
fx_df <- data.frame(fx)
for (j in 2:N) {
fx[j] = (1/2*exp(-abs(x[j])))
}
fx_df <- data.frame(fx)
install.packages('ggplot')
library(ggplot2)
ggplot(fx_df, aes(x= x, y=after_stat(density))) +
geom_histogram(fill="steelblue", na.rm = TRUE) +
ggtitle("Distribution of x")
ggplot(fx_df, aes(x= x, y=after_stat(density))) +
geom_density(fill="steelblue", na.rm = TRUE) +
ggtitle("Distribution of x")
sample_mean = mean(fx_df$fx)
sample_sd = sd(fx_df$fx)
initial_x_j1 <- c(2) #Give x a value of 2
assign('N', 2000) #Set value of N as 2000
assign('s_j', 0.001) #Set initial value of s for j sequence as 0.001
x_star_j1 <- rnorm(n = 1, mean = initial_x_j1, sd = s_j)
r = (1/2*exp(-abs(x_star_j1)))/(1/2*exp(-abs(initial_x_j1)))
u = runif(1, min = 0, max = 1)
if (u < r) x_j1 = x_star_j1 else x_j1 = initial_x_j1
x_j1_df <- data.frame(x_j1)
for (i in 2:N) {
x_star_j1 <- rnorm(n = 1, mean = x_j1[i-1], sd = s_j)
r <- (1/2*exp(-abs(x_star_j1)))/(1/2*exp(-abs(x_j1[i-1])))
u <- runif(1, min = 0, max = 1)
if (u < r) x_j1[i] = x_star_j1 else x_j1[i] = x_j1[i-1]
}
x_j1_df <- data.frame(x_j1)
M_1 <- mean(x_j1_df$x_j1)
M_1
V_1 <- 1/N*(((x_j1[1]- M_1)**2))
V_1_df <- data.frame(V_1)
for (i in 2:N) {
V_1[i] <- 1/N*(sum((x_j1[i]- M_1)**2))
}
V_1_df <- data.frame(V_1)
initial_x_j2 <- c(3) #Give x a value of 3
assign('N', 2000) #Set value of N as 2000
assign('s_j', 0.001) #Set initial value of s for j sequence as 0.001
x_star_j2 <- rnorm(n = 1, mean = initial_x_j2, sd = s_j)
r = (1/2*exp(-abs(x_star_j2)))/(1/2*exp(-abs(initial_x_j2)))
u = runif(1, min = 0, max = 1)
if (u < r) x_j2 = x_star_j2 else x_j2 = initial_x_j2
x_j2_df <- data.frame(x_j2)
for (i in 2:N) {
x_star_j2 <- rnorm(n = 1, mean = x_j2[i-1], sd = s_j)
r <- (1/2*exp(-abs(x_star_j2)))/(1/2*exp(-abs(x_j2[i-1])))
u <- runif(1, min = 0, max = 1)
if (u < r) x_j2[i] = x_star_j2 else x_j2[i] = x_j2[i-1]
}
x_j2_df <- data.frame(x_j2)
M_2 <- mean(x_j2_df$x_j2)
M_2
V_2 <- 1/N*(((x_j2[1]- M_2)**2))
V_2_df <- data.frame(V_2)
for (i in 2:N) {
V_2[i] <- 1/N*(sum((x_j2[i]- M_2)**2))
}
V_2_df <- data.frame(V_2)
initial_x_j3 <- c(4) #Give x a value of 3
assign('N', 2000) #Set value of N as 2000
assign('s_j', 0.001) #Set initial value of s for j sequence as 0.001
x_star_j3 <- rnorm(n = 1, mean = initial_x_j3, sd = s_j)
r = (1/2*exp(-abs(x_star_j3)))/(1/2*exp(-abs(initial_x_j3)))
u = runif(1, min = 0, max = 1)
if (u < r) x_j3 = x_star_j3 else x_j3 = initial_x_j3
x_j3_df <- data.frame(x_j3)
for (i in 2:N) {
x_star_j3 <- rnorm(n = 1, mean = x_j3[i-1], sd = s_j)
r <- (1/2*exp(-abs(x_star_j3)))/(1/2*exp(-abs(x_j3[i-1])))
u <- runif(1, min = 0, max = 1)
if (u < r) x_j3[i] = x_star_j3 else x_j3[i] = x_j3[i-1]
}
x_j3_df <- data.frame(x_j3)
x_j3_df <- data.frame(x_j3)
M_3 <- mean(x_j3_df$x_j3)
M_3
V_3 <- 1/N*(((x_j3[1]- M_3)**2))
V_3_df <- data.frame(V_3)
for (i in 2:N) {
V_3[i] <- 1/N*(sum((x_j3[i]- M_3)**2))
}
V_3_df <- data.frame(V_3)
initial_x_j4 <- c(5) #Give x a value of 3
assign('N', 2000) #Set value of N as 2000
assign('s_j', 0.001) #Set initial value of s for j sequence as 0.001
x_star_j4 <- rnorm(n = 1, mean = initial_x_j4, sd = s_j)
r = (1/2*exp(-abs(x_star_j4)))/(1/2*exp(-abs(initial_x_j4)))
u = runif(1, min = 0, max = 1)
if (u < r) x_j4 = x_star_j4 else x_j4 = initial_x_j4
x_j4_df <- data.frame(x_j4)
for (i in 2:N) {
x_star_j4 <- rnorm(n = 1, mean = x_j4[i-1], sd = s_j)
r <- (1/2*exp(-abs(x_star_j4)))/(1/2*exp(-abs(x_j4[i-1])))
u <- runif(1, min = 0, max = 1)
if (u < r) x_j4[i] = x_star_j4 else x_j4[i] = x_j4[i-1]
}
x_j4_df <- data.frame(x_j4)
M_4 <- mean(x_j4_df$x_j4)
M_4
V_4 <- 1/N*(((x_j4[1]- M_4)**2))
V_4_df <- data.frame(V_4)
for (i in 2:N) {
V_4[i] <- 1/N*(sum((x_j4[i]- M_4)**2))
}
V_4_df <- data.frame(V_4)
J <- 4
W = 1/J*(sum(V_1)+sum(V_2)+sum(V_3)+sum(V_4))
W
M = 1/J*(sum(M_1)+sum(M_2)+sum(M_3)+sum(M_4))
M
B <- 1/J*(((M_1- M)**2)+((M_2- M)**2)+((M_3- M)**2)+((M_4- M)**2))
for (i in 1:N)
B = (1/J*(sum((M_[i]- M)**2)))
R <- sqrt((B+W)/W)
R
install.packages(c("askpass", "backports", "bbotk", "bit", "bit64", "bitops", "boot", "broom", "bslib", "cachem", "callr", "caret", "caTools", "checkmate", "class", "cli", "clock", "cluster", "codetools", "colorspace", "commonmark", "cpp11", "crayon", "curl", "data.table", "DBI", "dbplyr", "digest", "e1071", "evaluate", "fansi", "farver", "fastmap", "fontawesome", "foreign", "fs", "future", "future.apply", "glue", "gower", "gtable", "hardhat", "highr", "htmltools", "ipred", "jsonlite", "KernSmooth", "knitr", "lattice", "lava", "lifecycle", "lubridate", "markdown", "mgcv", "mlbench", "mlr3", "mlr3learners", "mlr3measures", "mlr3misc", "mlr3pipelines", "mlr3tuning", "mlr3viz", "modeldata", "munsell", "nlme", "nnet", "openssl", "paradox", "parallelly", "pillar", "processx", "prodlim", "progressr", "ps", "purrr", "R6", "ragg", "Rcpp", "RcppEigen", "recipes", "reprex", "rlang", "rmarkdown", "rpart", "RSQLite", "rstudioapi", "rvest", "slider", "sparklyr", "spatial", "stringi", "stringr", "survival", "sys", "systemfonts", "textshaping", "timeDate", "tinytex", "utf8", "uuid", "vctrs", "withr", "xfun", "xgboost", "xml2", "yaml"))
library('MASS')   # load data from MASS library
df<-Boston
library('MASS')   # load data from MASS library
df<-Boston
library('mlr3')
task <- TaskRegr$new('boston', backend=df, target='medv')   # Setup regression task, id='boston'
measure <- msr('regr.mse')                                  # aggregate measure
library('mlr3learners')
learner_lm <- lrn('regr.lm')    # learner: linear regression
library('mlr3pipelines')
gr_lm <- po('imputemean') %>>% po(learner_lm)   # PipeOp: imputemean; resulting pipeline: graph
glrn_lm <- GraphLearner$new(gr_lm)              # combined learner: glrn_lm
set.seed(1)                                         # for reproducible results
train_set <- sample(task$nrow, 0.7 * task$nrow)     # setup train set
test_set <- setdiff(seq_len(task$nrow), train_set)  # setup test set
glrn_lm$train(task, row_ids = train_set)            # training
glrn_lm$predict(task, row_ids = test_set)$score()   # extracting predictions; evaluating performance/mse
learner_ridge <- lrn('regr.glmnet')
learner_ridge$param_set$values <- list(alpha = 0, lambda = 0.03)
gr_ridge <- po('scale') %>>%
po('imputemean') %>>%
po(learner_ridge)
glrn_ridge<- GraphLearner$new(gr_ridge)
glrn_ridge$train(task, row_ids = train_set)
glrn_ridge$predict(task, row_ids = test_set)$score()
library('mlr3tuning')
library('paradox')
tune_lambda <- ParamSet$new (list(
ParamDbl$new('regr.glmnet.lambda', lower = 0.001, upper = 2)
))                                        # Specify hyperparameters and the range to be explored
tuner<-tnr('grid_search')                 # Determine how the search will be done
terminator <- trm('evals', n_evals = 20)  # Determine how many evaluations to be repeated
learner_ridge2 <- lrn('regr.glmnet')
learner_ridge2$param_set$values <- list(alpha = 0)
gr_ridge2 <- po('scale') %>>%
po('imputemean') %>>%
po(learner_ridge2)
glrn_ridge2 <- GraphLearner$new(gr_ridge2)
at_ridge <- AutoTuner$new(
learner = glrn_ridge2,
resampling = rsmp('cv', folds = 3),
measure = measure,
search_space = tune_lambda,
terminator = terminator,
tuner = tuner
)                                         # cv: cross-validation
at_ridge$train(task, row_ids = train_set)
at_ridge$predict(task, row_ids = test_set)$score()
tune_lambda <- ParamSet$new(p_dbl(lower = 0.001, upper = 2))
tune_lambda <- ParamSet$new(x = p_dbl(lower = 0.001, upper = 2))
tune_lambda <- ParamSet(x = p_dbl(lower = 0.001, upper = 2))
# First Set up tuning environment
tune_lambda <- ps(x = p_dbl(lower = 0.001, upper = 2))
tuner<-tnr('grid_search')                 # Determine how the search will be done
terminator <- trm('evals', n_evals = 20)  # Determine how many evaluations to be repeated
learner_ridge2 <- lrn('regr.glmnet')
learner_ridge2$param_set$values <- list(alpha = 0)
gr_ridge2 <- po('scale') %>>%
po('imputemean') %>>%
po(learner_ridge2)
glrn_ridge2 <- GraphLearner$new(gr_ridge2)
at_ridge <- AutoTuner$new(
learner = glrn_ridge2,
resampling = rsmp('cv', folds = 3),
measure = measure,
search_space = tune_lambda,
terminator = terminator,
tuner = tuner
)                                         # cv: cross-validation
at_ridge$train(task, row_ids = train_set)
warnings()
learner_ridge2 <- lrn('regr.glmnet')
learner_ridge2$param_set$values <- list(alpha = 0)
gr_ridge2 <- po('scale.scale') %>>%
po('imputemean') %>>%
po(learner_ridge2)
glrn_ridge2 <- GraphLearner$new(gr_ridge2)
tune_lambda <- ps(regr.glmnet.lambda = p_dbl(lower = 0.001, upper = 2))
tuner<-tnr('grid_search')                 # Determine how the search will be done
terminator <- trm('evals', n_evals = 20)  # Determine how many evaluations to be repeated
learner_ridge2 <- lrn('regr.glmnet')
learner_ridge2$param_set$values <- list(alpha = 0)
gr_ridge2 <- po('scale.scale') %>>%
po('imputemean') %>>%
po(learner_ridge2)
glrn_ridge2 <- GraphLearner$new(gr_ridge2)
learner_ridge2 <- lrn('regr.glmnet')
learner_ridge2$param_set$values <- list(alpha = 0)
gr_ridge2 <- po('scale') %>>%
po('imputemean') %>>%
po(learner_ridge2)
glrn_ridge2 <- GraphLearner$new(gr_ridge2)
at_ridge <- AutoTuner$new(
learner = glrn_ridge2,
resampling = rsmp('cv', folds = 3),
measure = measure,
search_space = tune_lambda,
terminator = terminator,
tuner = tuner
)                                         # cv: cross-validation
at_ridge$train(task, row_ids = train_set)
at_ridge$predict(task, row_ids = test_set)$score()
setwd("/Volumes/SANDISK512/UOL/ST2195/Practice Assignments/Practice Assignment 02 (2025)/r_csv")
sim_html <- read_html("https://www.sim.edu.sg")
if (!("rvest" %in% installed.packages())) {
install.packages("rvest")
}
library(rvest)
sim_html <- read_html("https://www.sim.edu.sg")
sim_html
sim_nodes <- sim_html %>%
html_elements(xpath = '//footer//div[contains(.,"contact-item"//span)]')
#csv_tables <- csv_wiki %>% html_nodes(xpath = '//h2[contains(.,"Example")]//following-sibling::pre') %>% html_text()
sim_nodes
sim_nodes <- sim_html %>%
html_elements(xpath = '//footer//div[contains(.,"contact-item")]//span')
#csv_tables <- csv_wiki %>% html_nodes(xpath = '//h2[contains(.,"Example")]//following-sibling::pre') %>% html_text()
sim_nodes
sim_nodes <- sim_html %>%
html_elements(xpath = '//footer//div[contains(@class,"contact-item")]//span')
#csv_tables <- csv_wiki %>% html_nodes(xpath = '//h2[contains(.,"Example")]//following-sibling::pre') %>% html_text()
sim_nodes
sim_text <- sim_nodes %>% html_text()
sim_text
